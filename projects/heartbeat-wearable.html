<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Would You Understand – Thuy Dung Nguyen</title>
    <link rel="icon" type="image/svg+xml" href="../assets/logo.svg">
    <link rel="stylesheet" href="../style.css">
    <style>
        .context-box { background: #f8f8f8; padding: 16px; border-radius: 6px; margin: 28px 0; line-height: 1.11.88; }
        .context-box p { margin-bottom: 8px; }
        .challenge-highlight { border-left: 4px solid #0a0a0a; padding-left: 16px; margin: 24px 0; }
        .challenge-highlight p { margin-bottom: 16px; }
        .section-divider { height: 1px; background: #ddd; margin: 48px 0; }
        .stats-row { display: flex; gap: 40px; margin: 32px 0; flex-wrap: wrap; }
        .stat-item { flex: 1; min-width: 200px; }
        .stat-label { font-size: 14px; color: #888; font-weight: 600; }
        .stat-value { font-size: 24px; font-weight: 700; color: #0a0a0a; margin-top: 4px; }
        ul { margin: 16px 0 20px 20px; line-height: 1.8; }
        li { margin-bottom: 10px; }
    </style>
</head>
<body>
    <div class="layout">
        <aside class="sidebar">
            <div class="sidebar-content">
                <a href="../index.html" class="logo">
                    <img src="../assets/logo.svg" alt="Logo">
                </a>
                <nav class="nav">
                    <a href="../index.html#work">Work</a>
                    <a href="../index.html#about">About</a>
                    <a href="../index.html#contact">Contact</a>
                </nav>
            </div>
        </aside>
        
        <main class="content">
            <a href="../index.html" style="font-size: 14px; color: #888888; text-decoration: none; margin-bottom: 40px; display: inline-block;">← Back</a>
            
            <h1>Would You Understand</h1>
            
            <div style="max-width: 900px;">
                <p style="font-size: 20px; color: #555; line-height: 1.8; margin: 24px 0;">A two-person heartbeat-sharing device that creates physical empathy through real-time haptic feedback, exploring whether feeling another's heartbeat can build understanding when words fail.</p>
                
                <div class="context-box">
                    <p><strong>Course:</strong> Independent Research (Delta Residency) | <strong>Timeline:</strong> 3 weeks | <strong>Team:</strong> Solo | <strong>Status:</strong> Functional Prototype (Dual-Person System)</p>
                    <p><strong>Technologies:</strong> Python (OpenCV, NumPy, SciPy), Arduino Mega 2560, rPPG, haptic motors, HC-05 Bluetooth</p>
                </div>
                
                <div class="section-divider"></div>
                
                <h2 style="margin-top: 30px; margin-bottom: 12px;">The Question</h2>
                <p>Can physically feeling another person's heartbeat increase empathy and emotional connection when words fail?</p>
                <p>In moments of emotional overwhelm, social stress, or communication barriers, we often retreat into silence. Yet our hearts continue to beat—a rhythm that speaks louder than any explanation. This project investigates whether transmitting one person's heartbeat as tactile vibrations to another can create a channel for understanding that transcends language and operates at a physiological level.</p>
                
                <h2 style="margin-top: 30px; margin-bottom: 12px;">The Approach</h2>
                <p>Rather than displaying heartbeat data visually or aurally, I wanted to create direct physical feeling—translating one person's heart rhythm into haptic vibrations felt by another. This required solving three major technical and conceptual challenges:</p>
                
                <div class="challenge-highlight">
                    <p><strong>Challenge 1: Contactless Detection</strong> – Wired sensors on fingers or chest break intimacy and limit interaction. <strong>Solution:</strong> Implemented smartphone camera-based remote photoplethysmography (rPPG) that detects blood flow color changes in facial skin—no sensors needed.</p>
                </div>
                
                <div class="challenge-highlight">
                    <p><strong>Challenge 2: Realistic Haptic Sensation</strong> – Raw sensor-to-motor mapping creates erratic, clinical-feeling vibrations. <strong>Solution:</strong> Developed temporal smoothing and beat interpolation to generate physiologically plausible "lub-dub" patterns, making the sensation feel like a real heartbeat rather than robotic pulses.</p>
                </div>
                
                <div class="challenge-highlight">
                    <p><strong>Challenge 3: Dual-Person Synchronization</strong> – Tracking and transmitting two people's heart rates simultaneously while maintaining low latency is non-trivial. <strong>Solution:</strong> Face cascade classifier with size-based sorting, dual independent signal buffers, and serial communication via Arduino.</p>
                </div>
                
                <div class="section-divider"></div>
                
                <h2 style="margin-top: 30px; margin-bottom: 12px;">Technical Implementation</h2>
                <p><strong>Hardware:</strong> Arduino Mega 2560, 2x Coin Vibration Motors (1027 type, 3V), MacBook Webcam (720p, 30fps), External 5V Power Supply, HC-05 Bluetooth Module</p>
                <p><strong>Software:</strong> Python 3.13 with OpenCV (face detection), NumPy (signal processing), SciPy (FFT), PySerial (Arduino communication); Arduino C++ with PWM motor control, dual-motor timing, and LED feedback</p>
                
                <div class="stats-row">
                    <div class="stat-item">
                        <div class="stat-label">Detection Accuracy</div>
                        <div class="stat-value">±3 BPM</div>
                        <p style="font-size: 14px; color: #888; margin-top: 8px;">vs clinical reference</p>
                    </div>
                    <div class="stat-item">
                        <div class="stat-label">Update Frequency</div>
                        <div class="stat-value">2 Hz</div>
                        <p style="font-size: 14px; color: #888; margin-top: 8px;">BPM recalculation rate</p>
                    </div>
                    <div class="stat-item">
                        <div class="stat-label">Latency</div>
                        <div class="stat-value">1-2 sec</div>
                        <p style="font-size: 14px; color: #888; margin-top: 8px;">face detection to haptics</p>
                    </div>
                </div>
                
                <div class="section-divider"></div>
                
                <h2 style="margin-top: 30px; margin-bottom: 12px;">Process & Iteration</h2>
                <p><strong>Weeks 1-3:</strong> Researched rPPG algorithms, built Arduino motor tests, implemented basic face detection + FFT pipeline. <strong>Breakthrough:</strong> Discovered direct motor connection works with external power (no transistor needed).</p>
                <p><strong>Week 4 Critical Debugging:</strong> Serial communication breakdown—Python sending data but Arduino not receiving. Root cause: Arduino IDE Serial Monitor was locking the port. Solution: Close IDE completely before running Python.</p>
                <p><strong>Weeks 5-7:</strong> Modified code for independent dual-motor control. Added LED feedback system for silent operation confirmation. Implemented 3-second timeout to stop motors when no face detected.</p>
                <p><strong>Weeks 8-10:</strong> Validated dual-motor system, tested with reference pulse oximeter (±3 BPM accuracy confirmed), prepared comprehensive documentation.</p>
                <p><strong>Weeks 11-12:</strong> Final testing with actual users, acquired HC-05 Bluetooth module for wireless next iteration, focused on UX rather than technical complexity.</p>
                
                <div class="section-divider"></div>
                
                <h2 style="margin-top: 30px; margin-bottom: 12px;">Outcome & Reflection</h2>
                <p>Successfully created a functional prototype that generates immediate, intuitive heartbeat sensation. Key findings:</p>
                <ul style="line-height: 2;">
                    <li>rPPG accuracy consistently within ±3 BPM of clinical reference</li>
                    <li>Real-time latency (1-2 sec) feels immediate and responsive to users</li>
                    <li>"Lub-dub" vibration pattern recognized as heartbeat immediately without explanation</li>
                    <li>Dual-person detection works reliably with two faces simultaneously</li>
                </ul>
                
                <p style="margin-top: 24px;"><strong>Key Insight:</strong> The technical challenge wasn't the algorithm—it was the interface. The biggest breakthroughs came from constraints that forced simplification: eliminating Serial Monitor dependency (LED feedback instead), auto-stopping when no face detected, simplifying data format for Arduino parsing. The system works best when it "disappears"—when users stop thinking about the technology and just experience the sensation.</p>
                
                <p><strong>Next Steps:</strong> Conduct structured user study with 10+ participant pairs, implement HC-05 Bluetooth wireless communication, design soft silicone/fabric enclosures for comfortable wear, explore cross-distance heartbeat sharing (internet-based), develop empathy measurement protocol (IRI scale, qualitative interviews), adapt for accessibility (non-visual perception for deaf/blind users).</p>
                
                <div class="section-divider"></div>
                
                <h2 style="margin-top: 30px; margin-bottom: 12px;">Gallery</h2>
                <p><em>Dual-face detection screenshots, Arduino setup with motors, BPM data stream terminal output, LED confirmation flashing, close-up motor vibration patterns, user testing sessions</em></p>
                
                <h2 style="margin-top: 30px; margin-bottom: 12px;">Documentation</h2>
                <p><a href="https://github.com/julxng/would-you-understand" style="color: #0a0a0a; text-decoration: underline;">GitHub Repository</a> – Complete source code, setup instructions, troubleshooting guide</p>
                <p><a href="https://github.com/julxng/would-you-understand/wiki" style="color: #0a0a0a; text-decoration: underline;">Technical Wiki</a> – Detailed architecture, algorithm explanations, user testing protocol</p>
                
                <h2 style="margin-top: 30px; margin-bottom: 12px;">References</h2>
                <ul>
                    <li>Goldstein et al. (2017): Brain-to-brain coupling during handholding</li>
                    <li>Verkruysse et al. (2008): Remote photoplethysmographic imaging</li>
                    <li>Jakubiak & Feeney (2017): Affectionate touch and cardiovascular reactivity</li>
                </ul>
            </div>
        </main>
    </div>
</body>
</html>
